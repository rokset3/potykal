{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparing repo :D","metadata":{}},{"cell_type":"code","source":"%%writefile model.py\n\nimport os\nimport random\nimport math\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nfrom einops import rearrange, reduce, repeat\nfrom torch import einsum \n\nimport numpy as np\n\n\n\nclass GPT(nn.Module):\n    def __init__(self,\n                 vocab_size,\n                 num_layers,\n                 num_heads,\n                 hidden_dim,\n                 ffc_hidden_dim,\n                 attn_dropout_p=0.1,\n                 ffc_dropout_p=0.1,\n                 max_seq_len=512,\n                 ):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.ffc_hidden_dim = ffc_hidden_dim\n        self.attn_dropout_p = attn_dropout_p\n        self.ffc_dropout_p = ffc_dropout_p\n        self.max_seq_len = max_seq_len\n\n        self.decoder_block = nn.ModuleList([DecoderLayer(self.num_heads,\n                                                         self.hidden_dim,\n                                                         self.ffc_hidden_dim,\n                                                         self.attn_dropout_p,\n                                                         self.ffc_dropout_p) for _ in range(self.num_layers)])\n        \n        self.pos_embeddings = nn.Embedding(self.max_seq_len, self.hidden_dim)\n        self.token_embeddings = nn.Embedding(self.vocab_size, self.hidden_dim)\n\n        self.proj_layer = nn.Linear(self.hidden_dim, self.vocab_size)\n        \n        self.register_buffer('tril',\n                             torch.tril(torch.ones(self.max_seq_len, self.max_seq_len)).bool())\n        self.register_buffer('pos_ids',\n                             torch.arange(self.max_seq_len))\n    \n\n\n    def forward(self,\n                input_tokens,\n                tokenizer_mask=None):\n        seq_len = input_tokens.shape[-1]\n        b_size = input_tokens.shape[0]\n        \n        mask = self.make_attn_mask(seq_len, b_size, tokenizer_mask)\n        \n        x = self.pos_embeddings(self.pos_ids[:seq_len]) + self.token_embeddings(input_tokens)\n\n        for layer in self.decoder_block:\n            x = layer(x)\n        x = self.proj_layer(x)\n        return x\n\n\n    def make_attn_mask(self, seq_len, b_size, tokenizer_mask=None):\n        mask = self.tril[:seq_len, :seq_len].unsqueeze(0).repeat(b_size, 1, 1)\n        \n        if tokenizer_mask is not None:\n            mask = mask & tokenizer_mask.bool().unsqueeze(1)\n        return mask\n\nclass MSALayer(nn.Module):\n    def __init__(self,\n                 num_heads,\n                 hidden_dim,\n                 attn_dropout_p=0.1\n                 ):\n        \n        assert hidden_dim % num_heads == 0\n        \n        super().__init__()\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.head_dim = self.hidden_dim // self.num_heads\n        self.attn_dropout_p = attn_dropout_p\n\n        self.toq = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.tok = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.tov = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.ffc = nn.Linear(self.hidden_dim, self.hidden_dim)\n\n        self.layer_norm = nn.LayerNorm(normalized_shape=self.hidden_dim)\n        self.attn_dropout = nn.Dropout(p=self.attn_dropout_p if self.attn_dropout_p else 0)\n\n    def forward(self,\n                x,\n                mask=None):\n        # shape of input is [b_size, seq_len, hidden_dim]\n        q = self.toq(x)\n        k = self.tok(x)\n        v = self.tov(x)\n        \n        q = rearrange(q, 'b s (num_heads h) -> (b num_heads) s h', num_heads=self.num_heads)\n        k = rearrange(k, 'b s (num_heads h) -> (b num_heads) s h', num_heads=self.num_heads)\n        v = rearrange(v, 'b s (num_heads h) -> (b num_heads) s h', num_heads=self.num_heads)\n\n        output, probs = attn_function(q, k, v, mask=mask, attn_dropout=self.attn_dropout)\n\n        output = rearrange(output, '(b num_heads) s h -> b s (num_heads h)', num_heads=self.num_heads)\n        output = self.ffc(output)\n\n        output = self.layer_norm(output + x)\n        return output, probs\n        \n        \nclass DecoderLayer(nn.Module):\n    def __init__(self, \n                 num_heads,\n                 hidden_dim,\n                 ffc_hidden_dim,\n                 attn_dropout_p=0.1,\n                 ffc_dropout_p=0.1,\n                 ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.ffc_hidden_dim = ffc_hidden_dim\n        self.attn_dropout_p = attn_dropout_p\n        self.ffc_dropout_p = ffc_dropout_p\n\n        self.ffc_layer = nn.Sequential(\n            nn.Linear(self.hidden_dim, self.ffc_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(p=self.ffc_dropout_p),\n            nn.Linear(self.ffc_hidden_dim, self.hidden_dim)\n        ) \n        self.ffc_layer_norm = nn.LayerNorm(normalized_shape=self.hidden_dim)\n\n        self.msalayer = MSALayer(self.num_heads,\n                                 self.hidden_dim,\n                                 self.attn_dropout_p,)\n\n    def forward(self,\n                x,\n                mask=None):\n        res = x\n        x, _ = self.msalayer(x, mask=mask)\n        \n        return self.ffc_layer_norm(self.ffc_layer(x) + res)\n    \n\ndef attn_function(q, k, v, mask=None, attn_dropout=None):\n    \n    #q, k, v shape is [b, s, h]\n    b_size = q.shape[0]\n    seq_len = q.shape[1]\n    hidden_dim = q.shape[2]\n\n\n    scaled_dot_product = einsum('bsh, bvh -> bsv', [q, k])/math.sqrt(hidden_dim)\n\n    if mask:\n        scaled_dot_product = scaled_dot_product.masked_fill(mask==False, 1e-9)\n    \n    if attn_dropout:\n        scaled_dot_product = attn_dropout(scaled_dot_product)\n    \n    attn_probs = F.softmax(scaled_dot_product, dim=-1)\n    attn_output = einsum('bsv, bvd -> bsd', [attn_probs, v])\n\n    return attn_output, attn_probs","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-21T01:16:21.168235Z","iopub.execute_input":"2023-07-21T01:16:21.168761Z","iopub.status.idle":"2023-07-21T01:16:21.189249Z","shell.execute_reply.started":"2023-07-21T01:16:21.168730Z","shell.execute_reply":"2023-07-21T01:16:21.188274Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Writing model.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile train_tokenizer.py\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.processors import TemplateProcessing\n\nif __name__ == \"__main__\":\n    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n    trainer = BpeTrainer(special_tokens=[\"<unk>\",\n                                         \"<s>\",\n                                         \"<pad>\",\n                                         \"<bos>\",\n                                         ], vocab_size=10000) #i took 10k just randomly\n    tokenizer.pre_tokenizer = Whitespace()\n\n    files = [\"data/input.txt\"]\n    tokenizer.train(files, trainer)\n\n    tokenizer.post_processor = TemplateProcessing(\n        single=\"<bos> $A <s>\",\n        special_tokens=[\n            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n            (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n            (\"<pad>\", tokenizer.token_to_id(\"<pad>\"))\n        ],\n    )\n    tokenizer.enable_padding(pad_id=2, pad_token=\"<pad>\")\n    tokenizer.save(\"data/tokenizer.json\")","metadata":{"execution":{"iopub.status.busy":"2023-07-21T01:18:06.520541Z","iopub.execute_input":"2023-07-21T01:18:06.520925Z","iopub.status.idle":"2023-07-21T01:18:06.529803Z","shell.execute_reply.started":"2023-07-21T01:18:06.520892Z","shell.execute_reply":"2023-07-21T01:18:06.528670Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Overwriting train_tokenizer.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile objects.py\n\nimport numpy as np\nimport torch\n\nfrom torch.utils.data import Dataset\nfrom tokenizers import Tokenizer\nfrom typing import List\n\n\nclass ConstantLenghtDataset(Dataset):\n    def __init__(self, \n                 texts: List[str],\n                 tokenizer: Tokenizer,\n                 length: int=512,):\n        self.texts = texts\n        self.length = length\n        self.tokenizer = tokenizer\n        self.tokenizer.no_padding()\n\n        encoded_text = tokenizer.encode_batch(self.texts)\n        tokens_num = [len(s.tokens) for s in encoded_text]\n        constant_len_dataset_ids = []\n        concat_sentences_ids = []\n        sum=0\n        \n        for idx, num in enumerate(tokens_num):\n            if sum > 512:\n                constant_len_dataset_ids.append(concat_sentences_ids)\n                concat_sentences_ids = []\n                sum = 0\n\n            concat_sentences_ids.append(idx)\n            sum+=num\n        \n        np_text = np.array(self.texts)\n        new_dataset = []\n        for idxs in constant_len_dataset_ids:\n            new_dataset.append(' '.join(np_text[idxs].tolist()))\n\n        self.dataset = new_dataset\n\n    def __len__(self,):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        return self.dataset[idx]\n    \nclass TokenizerWrapper():\n    def __init__(self,\n                 tokenizer,\n                 pad_seq_len=512):\n        self.tokenizer = tokenizer\n        self.pad_seq_len = pad_seq_len\n        self.tokenizer.enable_padding(pad_id=2, pad_token=\"<pad>\", length=pad_seq_len)\n        self.vocab_size = self.tokenizer.get_vocab_size()\n\n    def __call__(self, input_sentences: List[str], batch=True):\n        output = {}\n        if batch:\n            encoded_input = self.tokenizer.encode_batch(input_sentences)\n            ids = torch.tensor([input.ids for input in encoded_input], requires_grad=False)\n            attn_masks = torch.tensor([input.attention_mask for input in encoded_input], requires_grad=False)\n        else:\n            encoded_input = self.tokenizer.encode(input_sentences)\n            ids = torch.tensor(encoded_input.ids, requires_grad=False).unsqueeze(0)\n            attn_masks = torch.tensor(encoded_input.attention_mask, requires_grad=False).unsqueeze(0)\n            \n        output['input_ids'] = ids\n        output['attn_mask'] = attn_masks\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-07-21T01:46:07.821609Z","iopub.execute_input":"2023-07-21T01:46:07.822442Z","iopub.status.idle":"2023-07-21T01:46:07.831652Z","shell.execute_reply.started":"2023-07-21T01:46:07.822401Z","shell.execute_reply":"2023-07-21T01:46:07.830575Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Overwriting objects.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile utils.py\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\n\nfrom tqdm import tqdm\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '8080'\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    \ndef cleanup():\n    dist.destroy_process_group()\n    \n    \ndef train(epoch, model, optimizer, train_dataloader, tokenizerwrapped):\n    model.train()\n    training_loss = 0\n    tokenizerwrapped.tokenizer.enable_padding(pad_id=2, pad_token=\"<pad>\", length=tokenizerwrapped.pad_seq_len)\n    for batch_num, batch in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        \n        inputs = tokenizerwrapped(batch)\n        labels = inputs['input_ids'].cuda(non_blocking=True)\n        attn_mask = inputs['attn_mask'].cuda(non_blocking=True)\n        \n        logits = model(labels, attn_mask)\n        \n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        \n        loss_fn = nn.CrossEntropyLoss()\n        loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))        \n        \n        loss.backward()\n        optimizer.step()\n        \n        training_loss += loss.item()\n\n    training_loss /= batch_num\n    print(f\"Epoch: {epoch}, Training loss: {training_loss}\")\n\ndef test(epoch, model, test_dataloader, tokenizerwrapped):\n    model.eval()\n    test_loss = 0\n    tokenizerwrapped.tokenizer.enable_padding(pad_id=2, pad_token=\"<pad>\", length=tokenizerwrapped.pad_seq_len)\n    with tqdm(total=len(test_dataloader.dataset)) as progress_bar:\n        with torch.no_grad():\n            for batch_idx, batch in enumerate(test_dataloader):\n                inputs = tokenizerwrapped(batch)\n                labels = inputs['input_ids'].cuda(non_blocking=True)\n                attn_mask = inputs['attn_mask'].cuda(non_blocking=True)\n\n                logits = model(labels, attn_mask)\n                \n                shift_logits = logits[..., :-1, :].contiguous()\n                shift_labels = labels[..., 1:].contiguous()\n        \n                loss_fn = nn.CrossEntropyLoss()\n                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n                \n                test_loss += loss.item()\n                progress_bar.update(labels.size(0))\n            \n            test_loss /= batch_idx\n    \n    return test_loss\n\n\ndef prepare_data():\n    with open('data/input.txt', 'r') as f:\n        input_text = f.readlines()\n    input_text = [i for i in input_text if i!='\\n']\n    train_size = 0.9\n    train_ids = int(len(input_text) * train_size)\n    train_data = input_text[: train_ids]\n    test_data = input_text[train_ids:]\n    \n    return train_data, test_data","metadata":{"execution":{"iopub.status.busy":"2023-07-21T01:52:06.589475Z","iopub.execute_input":"2023-07-21T01:52:06.590094Z","iopub.status.idle":"2023-07-21T01:52:06.598855Z","shell.execute_reply.started":"2023-07-21T01:52:06.590053Z","shell.execute_reply":"2023-07-21T01:52:06.597794Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Overwriting utils.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile main.py\nimport os\nimport sys\nfrom time import time_ns\n\nimport numpy as np\nimport random\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.backends.cudnn as cudnn\n\n\nfrom tokenizers import Tokenizer\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data import Dataset, DataLoader\n\n\nfrom model import GPT\nfrom objects import ConstantLenghtDataset, TokenizerWrapper\nfrom utils import (setup, cleanup, train, test, prepare_data)\n\n\ntokenizer = Tokenizer.from_file(\"data/tokenizer.json\")\nmodel_config = dict(\n    num_layers=12,\n    num_heads=12,\n    hidden_dim=768,\n    ffc_hidden_dim=3072,\n    max_seq_len=512,\n    vocab_size=tokenizer.get_vocab_size()\n)\n\ntokenizerwrapped = TokenizerWrapper(tokenizer, pad_seq_len=model_config['max_seq_len'])\n\n\n\n\n\ntrain_texts, test_texts = prepare_data()\n\ntrain_dataset = ConstantLenghtDataset(train_texts, tokenizer, length=model_config['max_seq_len'])\ntest_dataset = ConstantLenghtDataset(test_texts, tokenizer, length=model_config['max_seq_len'])\n\nNUM_EPOCHS = 50\nWORLD_SIZE = 2\nBATCH_SIZE = 16\nLR = 2e-5\nSAVE_INTERVAL=5\nSAVE_PATH = \"model.pt\"\n\n\n\n\ndef demo_basic(rank, world_size):\n    print(f\"Running basic GPT-1 traning on device: {rank}.\")\n    setup(rank, world_size)\n    \n    torch.cuda.set_device(rank)\n    train_sampler = DistributedSampler(\n        train_dataset,\n        num_replicas=world_size,\n        rank=rank,\n        shuffle=True\n    )\n    \n    train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,\n                              shuffle=False, num_workers=2, pin_memory=True, sampler=train_sampler)\n    \n    test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE,\n                            shuffle=False, num_workers=2, pin_memory=True)\n\n    \n    \n    model = GPT(**model_config).cuda(rank)\n    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, nesterov=True)\n    model = DDP(model, device_ids=[rank])\n    cudnn.benchmark = True\n    \n    for epoch in range(NUM_EPOCHS):\n        t0 = time_ns()\n\n        train(epoch, model, optimizer, train_dataloader, tokenizerwrapped)\n\n        t1 = time_ns()\n        delta = (t1 - t0) / (10 ** 9)\n        print(f\"Device {rank} - Train time: {delta} sec\")\n        \n        if rank == 0:\n            loss = test(epoch, model, test_dataloader, tokenizerwrapped)\n            print(f\"Loss: {loss}%\")\n            \n        if epoch % SAVE_INTERVAL == 0 and rank == 0:\n            torch.save(model.state_dict(), SAVE_PATH)\n        \n\n    cleanup()\n    \n    \ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)\n    \n    \nif __name__ == '__main__':\n    run_demo(demo_basic,\n             2)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T01:56:32.439616Z","iopub.execute_input":"2023-07-21T01:56:32.440020Z","iopub.status.idle":"2023-07-21T01:56:32.450825Z","shell.execute_reply.started":"2023-07-21T01:56:32.439987Z","shell.execute_reply":"2023-07-21T01:56:32.449738Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Overwriting main.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Inference is unfinished","metadata":{"execution":{"iopub.status.busy":"2023-07-21T03:07:32.343913Z","iopub.execute_input":"2023-07-21T03:07:32.344324Z","iopub.status.idle":"2023-07-21T03:07:32.351275Z","shell.execute_reply.started":"2023-07-21T03:07:32.344292Z","shell.execute_reply":"2023-07-21T03:07:32.350175Z"}}},{"cell_type":"code","source":"%%writefile inference.py\nimport torch\nfrom model import GPT\nfrom objects import TokenizerWrapper\nfrom tokenizers import Tokenizer\n\nif __name__ == \"__main__\":\n    prefix = \"<bos> A thou\"\n\n    tokenizer = Tokenizer.from_file(\"data/tokenizer.json\")\n    model_config = dict(\n        num_layers=12,\n        num_heads=12,\n        hidden_dim=768,\n        ffc_hidden_dim=3072,\n        max_seq_len=512,\n        vocab_size=tokenizer.get_vocab_size()\n    )\n    model = GPT(**model_config)\n    model.load_state_dict(torch.load('model.pt'))\n    model.eval()\n\n    tokenizerwrapped = TokenizerWrapper(tokenizer, 0)\n    batch = tokenizerwrapped(prefix, batch=False)\n\n    num_generations = 200\n    with torch.cuda.amp.autocast():\n        for i in range(num_generations):\n            attn_mask = batch['attn_mask']\n            outputs = model(batch['input_ids'].cuda(), attn_mask.cuda())\n            probs = outputs[0, -1].div(0.8).softmax(-1)\n            token = torch.multinomial(probs, 1).view([])\n\n            print(tokenizerwrapped.tokenizer.decode([token]), end=' ', flush=True)\n            batch = dict(input_ids=outputs[0, -1].argmax(-1).reshape(1, 1),\n                         attn_mask=torch.ones(num_generations, requires_grad=False).cuda()[:i+1])","metadata":{"execution":{"iopub.status.busy":"2023-07-21T02:47:09.708484Z","iopub.execute_input":"2023-07-21T02:47:09.709510Z","iopub.status.idle":"2023-07-21T02:47:09.717748Z","shell.execute_reply.started":"2023-07-21T02:47:09.709473Z","shell.execute_reply":"2023-07-21T02:47:09.716694Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Overwriting inference.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Code execution part","metadata":{}},{"cell_type":"code","source":"!pip install tokenizers\n!pip install einops","metadata":{"execution":{"iopub.status.busy":"2023-07-21T01:44:55.420964Z","iopub.execute_input":"2023-07-21T01:44:55.421920Z","iopub.status.idle":"2023-07-21T01:45:18.832940Z","shell.execute_reply.started":"2023-07-21T01:44:55.421874Z","shell.execute_reply":"2023-07-21T01:45:18.831605Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.13.3)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (0.6.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile get_data.sh\nmkdir data\ncd data\nwget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n!sh get_data.sh","metadata":{"execution":{"iopub.status.busy":"2023-07-21T01:18:04.503205Z","iopub.execute_input":"2023-07-21T01:18:04.503975Z","iopub.status.idle":"2023-07-21T01:18:04.510035Z","shell.execute_reply.started":"2023-07-21T01:18:04.503943Z","shell.execute_reply":"2023-07-21T01:18:04.509044Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Overwriting get_data.sh\n","output_type":"stream"}]},{"cell_type":"code","source":"!python train_tokenizer.py","metadata":{"execution":{"iopub.status.busy":"2023-07-21T01:18:09.879857Z","iopub.execute_input":"2023-07-21T01:18:09.880277Z","iopub.status.idle":"2023-07-21T01:18:11.948771Z","shell.execute_reply.started":"2023-07-21T01:18:09.880244Z","shell.execute_reply":"2023-07-21T01:18:11.947508Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[00:00:00] Pre-processing files (1 Mo)              ░░░░░░░░                  0%\n\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing files (1 Mo)              ██░░░░░░                 25%\n\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing files (1 Mo)              ████░░░░                 50%\n\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing files (1 Mo)              ██████░░                 75%\n\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing files (1 Mo)              ████████                100%\n[00:00:00] Tokenize words                           ████████ 0        /        0\n\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 13355    /    13355\n\n\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██████░░ 11571    /    13355\n\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████████ 13355    /    13355\n\n\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 500      /    10000\n\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ██░░░░░░ 3200     /    10000\n\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ██████░░ 7800     /    10000\n\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 9933     /     9933\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!python main.py","metadata":{"execution":{"iopub.status.busy":"2023-07-21T01:56:36.082454Z","iopub.execute_input":"2023-07-21T01:56:36.083189Z","iopub.status.idle":"2023-07-21T02:42:32.653869Z","shell.execute_reply.started":"2023-07-21T01:56:36.083152Z","shell.execute_reply":"2023-07-21T02:42:32.652271Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Running basic GPT-1 traning on device: 1.\n[W socket.cpp:601] [c10d] The client socket has failed to connect to [localhost]:8080 (errno: 99 - Cannot assign requested address).\nRunning basic GPT-1 traning on device: 0.\nEpoch: 0, Training loss: 9.899048646291098\nDevice 1 - Train time: 43.568003695 sec\nEpoch: 0, Training loss: 9.90012100007799\nDevice 0 - Train time: 43.579834161 sec\n100%|███████████████████████████████████████████| 62/62 [00:12<00:00,  5.11it/s]\nLoss: 12.43469206492106%\nEpoch: 1, Training loss: 9.775286303626167\nDevice 0 - Train time: 42.696818924 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 1, Training loss: 9.775092919667562\nDevice 1 - Train time: 55.631360185 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.44it/s]\nLoss: 12.24113686879476%\nEpoch: 2, Training loss: 9.629947503407797\nDevice 0 - Train time: 43.683513525 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 2, Training loss: 9.628786245981852\nDevice 1 - Train time: 55.080029132 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.39it/s]\nLoss: 12.039414087931315%\nEpoch: 3, Training loss: 9.480305088890923\nDevice 1 - Train time: 53.985724552 sec\nEpoch: 3, Training loss: 9.483031855689156\nDevice 0 - Train time: 42.545527942 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.33it/s]\nLoss: 11.83742650349935%\nEpoch: 4, Training loss: 9.335120730929905\nDevice 0 - Train time: 42.729411447 sec\nEpoch: 4, Training loss: 9.3327759636773\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Device 1 - Train time: 54.426595911 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.44it/s]\nLoss: 11.636590957641602%\nEpoch: 5, Training loss: 9.185811625586616\nDevice 1 - Train time: 54.158236934 sec\nEpoch: 5, Training loss: 9.189204163021511\nDevice 0 - Train time: 42.884323421 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 6, Training loss: 9.044696119096544\nDevice 0 - Train time: 43.580144703 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 6, Training loss: 9.039848115709093\nDevice 1 - Train time: 56.404794058 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.39it/s]\nLoss: 11.241100947062174%\nEpoch: 7, Training loss: 8.896565437316895\nDevice 1 - Train time: 54.074000572 sec\nEpoch: 7, Training loss: 8.90159241358439\nDevice 0 - Train time: 42.58092786 sec\n100%|███████████████████████████████████████████| 62/62 [00:12<00:00,  5.06it/s]\nLoss: 11.048800468444824%\nEpoch: 8, Training loss: 8.7575167549981\nDevice 1 - Train time: 55.45623668 sec\nEpoch: 8, Training loss: 8.762722810109457\nDevice 0 - Train time: 43.202077582 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.29it/s]\nLoss: 10.86324946085612%\nEpoch: 9, Training loss: 8.629738383822971\nDevice 0 - Train time: 43.598206853 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 9, Training loss: 8.623438358306885\nDevice 1 - Train time: 55.33838677 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.38it/s]\nLoss: 10.688825607299805%\nEpoch: 10, Training loss: 8.49911618232727\nDevice 1 - Train time: 54.941801466 sec\nEpoch: 10, Training loss: 8.505859348509047\nDevice 0 - Train time: 43.500296983 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.47it/s]\nLoss: 10.531749566396078%\nEpoch: 11, Training loss: 8.395956542756823\nDevice 0 - Train time: 42.996044491 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 11, Training loss: 8.389048973719278\nDevice 1 - Train time: 55.64836035 sec\n100%|███████████████████████████████████████████| 62/62 [00:12<00:00,  5.02it/s]\nLoss: 10.39869499206543%\nEpoch: 12, Training loss: 8.304314427905613\nDevice 0 - Train time: 42.688412222 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 12, Training loss: 8.297242747412788\nDevice 1 - Train time: 55.046708269 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.40it/s]\nLoss: 10.293526808420816%\nEpoch: 13, Training loss: 8.22551163037618\nDevice 1 - Train time: 55.192362364 sec\nEpoch: 13, Training loss: 8.23296226395501\nDevice 0 - Train time: 43.756667578 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.47it/s]\nLoss: 10.214914162953695%\nEpoch: 14, Training loss: 8.172088676028782\nDevice 1 - Train time: 54.271585269 sec\nEpoch: 14, Training loss: 8.179147879282633\nDevice 0 - Train time: 42.992041356 sec\n100%|███████████████████████████████████████████| 62/62 [00:12<00:00,  4.99it/s]\nLoss: 10.157458464304606%\nEpoch: 15, Training loss: 8.138941446940104\nDevice 0 - Train time: 42.757179569 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 15, Training loss: 8.131786929236519\nDevice 1 - Train time: 55.293951473 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.18it/s]\nLoss: 10.114885171254477%\nEpoch: 16, Training loss: 8.10111567709181\nDevice 1 - Train time: 56.184434462 sec\nEpoch: 16, Training loss: 8.109272559483847\nDevice 0 - Train time: 42.839055205 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.51it/s]\nLoss: 10.082141717274984%\nEpoch: 17, Training loss: 8.076955159505209\nDevice 1 - Train time: 54.451255503 sec\nEpoch: 17, Training loss: 8.085617515775892\nDevice 0 - Train time: 43.23145378 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.45it/s]\nLoss: 10.055771986643473%\nEpoch: 18, Training loss: 8.057307057910496\nDevice 1 - Train time: 54.525573184 sec\nEpoch: 18, Training loss: 8.066105312771267\nDevice 0 - Train time: 43.159418217 sec\n100%|███████████████████████████████████████████| 62/62 [00:12<00:00,  5.12it/s]\nLoss: 10.033565203348795%\nEpoch: 19, Training loss: 8.040428611967299\nDevice 1 - Train time: 55.025915104 sec\nEpoch: 19, Training loss: 8.048908525043064\nDevice 0 - Train time: 42.901747613 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.53it/s]\nLoss: 10.014125029246012%\nEpoch: 20, Training loss: 8.026043971379599\nDevice 1 - Train time: 54.593279537 sec\nEpoch: 20, Training loss: 8.035157044728598\nDevice 0 - Train time: 43.453963848 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.41it/s]\nLoss: 9.996584415435791%\nEpoch: 21, Training loss: 8.02150747511122\nDevice 0 - Train time: 42.959388186 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 21, Training loss: 8.01295616891649\nDevice 1 - Train time: 55.728785498 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.19it/s]\nLoss: 9.980360825856527%\nEpoch: 22, Training loss: 8.009172863430447\nDevice 0 - Train time: 42.789511453 sec\nEpoch: 22, Training loss: 8.000102202097574\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Device 1 - Train time: 54.729485751 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.43it/s]\nLoss: 9.965089162190756%\nEpoch: 23, Training loss: 7.988157855139838\nDevice 1 - Train time: 54.081382266 sec\nEpoch: 23, Training loss: 7.996830569373237\nDevice 0 - Train time: 42.701158882 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.43it/s]\nLoss: 9.950507640838623%\nEpoch: 24, Training loss: 7.97699072625902\nDevice 1 - Train time: 54.446467728 sec\nEpoch: 24, Training loss: 7.9856142732832165\nDevice 0 - Train time: 43.01956341 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.56it/s]\nLoss: 9.936432679494223%\nEpoch: 25, Training loss: 7.965636279847887\nDevice 1 - Train time: 53.767717472 sec\nEpoch: 25, Training loss: 7.974379857381185\nDevice 0 - Train time: 42.606899577 sec\n100%|███████████████████████████████████████████| 62/62 [00:12<00:00,  5.10it/s]\nLoss: 9.92273203531901%\nEpoch: 26, Training loss: 7.964561992221409\nDevice 0 - Train time: 42.990668274 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 26, Training loss: 7.955198314454821\nDevice 1 - Train time: 56.38095965 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.48it/s]\nLoss: 9.909311453501383%\nEpoch: 27, Training loss: 7.953987280527751\nDevice 0 - Train time: 43.428205802 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 27, Training loss: 7.944803953170776\nDevice 1 - Train time: 54.697371994 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.43it/s]\nLoss: 9.896098295847574%\nEpoch: 28, Training loss: 7.943913327323066\nDevice 0 - Train time: 42.738887608 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 28, Training loss: 7.9347648885515\nDevice 1 - Train time: 54.205934651 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.19it/s]\nLoss: 9.883033593495687%\nEpoch: 29, Training loss: 7.924285650253296\nDevice 1 - Train time: 54.552377101 sec\nEpoch: 29, Training loss: 7.933926264444987\nDevice 0 - Train time: 42.665410258 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.20it/s]\nLoss: 9.870075225830078%\nEpoch: 30, Training loss: 7.914526383082072\nEpoch: 30, Training loss: 7.923772388034397\nDevice 1 - Train time: 54.759889319 sec\nDevice 0 - Train time: 42.819092347 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.44it/s]\nLoss: 9.857187112172445%\nEpoch: 31, Training loss: 7.914328972498576\nDevice 0 - Train time: 43.344549844 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 31, Training loss: 7.904801474677192\nDevice 1 - Train time: 55.986276201 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.40it/s]\nLoss: 9.844339529673258%\nEpoch: 32, Training loss: 7.894594828287761\nDevice 1 - Train time: 54.132148971 sec\nEpoch: 32, Training loss: 7.904762585957845\nDevice 0 - Train time: 42.792716934 sec\n100%|███████████████████████████████████████████| 62/62 [00:12<00:00,  5.10it/s]\nLoss: 9.831509908040365%\nEpoch: 33, Training loss: 7.884692695405748\nDevice 1 - Train time: 54.832998891 sec\nEpoch: 33, Training loss: 7.89415619108412\nDevice 0 - Train time: 42.686440439 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.38it/s]\nLoss: 9.818679650624594%\nEpoch: 34, Training loss: 7.874855915705363\nDevice 1 - Train time: 54.855170779 sec\nEpoch: 34, Training loss: 7.884674469629924\nDevice 0 - Train time: 43.348004076 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.45it/s]\nLoss: 9.805832703908285%\nEpoch: 35, Training loss: 7.865360154045953\nDevice 1 - Train time: 53.992632822 sec\nEpoch: 35, Training loss: 7.87491054005093\nDevice 0 - Train time: 42.589490364 sec\n100%|███████████████████████████████████████████| 62/62 [00:12<00:00,  5.05it/s]\nLoss: 9.79295285542806%\nEpoch: 36, Training loss: 7.86496885617574\nDevice 0 - Train time: 42.941420251 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 36, Training loss: 7.855422761705187\nDevice 1 - Train time: 56.389720103 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.30it/s]\nLoss: 9.780031045277914%\nEpoch: 37, Training loss: 7.845662911732991\nDevice 1 - Train time: 54.339239123 sec\nEpoch: 37, Training loss: 7.85474157333374\nDevice 0 - Train time: 42.669238206 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.50it/s]\nLoss: 9.767051696777344%\nEpoch: 38, Training loss: 7.844971736272176\nDevice 0 - Train time: 43.282936344 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 38, Training loss: 7.834806521733602\nDevice 1 - Train time: 54.587028097 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.44it/s]\nLoss: 9.75401004155477%\nEpoch: 39, Training loss: 7.82558356391059\nDevice 1 - Train time: 54.203928531 sec\nEpoch: 39, Training loss: 7.835490730073717\nDevice 0 - Train time: 42.846603867 sec\n100%|███████████████████████████████████████████| 62/62 [00:12<00:00,  5.11it/s]\nLoss: 9.740894794464111%\nEpoch: 40, Training loss: 7.825187020831638\nDevice 0 - Train time: 42.663309714 sec\nEpoch: 40, Training loss: 7.81540388531155\nDevice 1 - Train time: 54.83701118 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.44it/s]\nLoss: 9.727702776590982%\nEpoch: 41, Training loss: 7.805207888285319\nDevice 1 - Train time: 55.992758334 sec\nEpoch: 41, Training loss: 7.815653350618151\nDevice 0 - Train time: 43.583979717 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.46it/s]\nLoss: 9.714423974355062%\nEpoch: 42, Training loss: 7.805000729031033\nDevice 0 - Train time: 42.555612605 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 42, Training loss: 7.79599912961324\nDevice 1 - Train time: 54.053614019 sec\n100%|███████████████████████████████████████████| 62/62 [00:12<00:00,  4.96it/s]\nLoss: 9.701051076253256%\nEpoch: 43, Training loss: 7.79503517680698\nDevice 0 - Train time: 42.922979791 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 43, Training loss: 7.785535679923163\nDevice 1 - Train time: 55.5583179 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.44it/s]\nLoss: 9.687580426534018%\nEpoch: 44, Training loss: 7.7747330400678845\nDevice 1 - Train time: 54.416542005 sec\nEpoch: 44, Training loss: 7.785124699274699\nDevice 0 - Train time: 43.188341493 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.42it/s]\nLoss: 9.674005349477133%\nEpoch: 45, Training loss: 7.774291223949856\nDevice 0 - Train time: 43.187726815 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 45, Training loss: 7.764071570502387\nDevice 1 - Train time: 54.69150914 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.36it/s]\nLoss: 9.660324732462565%\nEpoch: 46, Training loss: 7.754435035917494\nDevice 1 - Train time: 55.510155003 sec\nEpoch: 46, Training loss: 7.764249748653835\nDevice 0 - Train time: 42.869876229 sec\n100%|███████████████████████████████████████████| 62/62 [00:12<00:00,  5.09it/s]\nLoss: 9.646534442901611%\nEpoch: 47, Training loss: 7.753829929563734\nDevice 0 - Train time: 42.92723381 sec\n  0%|                                                    | 0/62 [00:00<?, ?it/s]Epoch: 47, Training loss: 7.743836879730225\nDevice 1 - Train time: 55.132044525 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.46it/s]\nLoss: 9.63262685139974%\nEpoch: 48, Training loss: 7.73270140753852\nDevice 1 - Train time: 55.012980285 sec\nEpoch: 48, Training loss: 7.742702510621813\nDevice 0 - Train time: 43.683823832 sec\n100%|███████████████████████████████████████████| 62/62 [00:11<00:00,  5.47it/s]\nLoss: 9.618602752685547%\nEpoch: 49, Training loss: 7.722662872738308\nDevice 1 - Train time: 54.343702196 sec\nEpoch: 49, Training loss: 7.7323925495147705\nDevice 0 - Train time: 43.076183989 sec\n100%|███████████████████████████████████████████| 62/62 [00:08<00:00,  7.29it/s]\nLoss: 9.604455153147379%\n","output_type":"stream"}]}]}