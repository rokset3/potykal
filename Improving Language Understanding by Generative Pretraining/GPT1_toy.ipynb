{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ca0bc58ebdd64a5f9322fb21c6ec9c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_340c8b2b866b456da6f82e25f6cbe651",
              "IPY_MODEL_060106b072c542aabddb41856367197e",
              "IPY_MODEL_89dfb97724f04a73a7004e02741f14b0"
            ],
            "layout": "IPY_MODEL_4083e851d32e4026bb49feb70b927d2b"
          }
        },
        "340c8b2b866b456da6f82e25f6cbe651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47384ddfbae043cf905a23de0d0732f3",
            "placeholder": "​",
            "style": "IPY_MODEL_84f6a474e2ac489da49b6eb9c5860be7",
            "value": "100%"
          }
        },
        "060106b072c542aabddb41856367197e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb432215a3b44f22beaaae7ae79b61a2",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c09b1ec7c63141d78b617ad4dd753d1d",
            "value": 10
          }
        },
        "89dfb97724f04a73a7004e02741f14b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea8f8b979b0444ae99ae1417e4b26af6",
            "placeholder": "​",
            "style": "IPY_MODEL_8535652bbfa94800b17459deee297c0a",
            "value": " 10/10 [09:55&lt;00:00, 59.44s/it]"
          }
        },
        "4083e851d32e4026bb49feb70b927d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47384ddfbae043cf905a23de0d0732f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84f6a474e2ac489da49b6eb9c5860be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb432215a3b44f22beaaae7ae79b61a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c09b1ec7c63141d78b617ad4dd753d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea8f8b979b0444ae99ae1417e4b26af6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8535652bbfa94800b17459deee297c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwV6TwUhibfa",
        "outputId": "393d217d-b566-4bfa-ea1c-589b7b93ea47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.22.4)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext) (1.26.16)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (16.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchtext) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchtext) (1.3.0)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install torchtext einops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT Paper replication\n",
        "* Notes for myself:\n",
        "\n",
        "    They used (msa->add->norm) -> (ffc -> add-> norm)\n",
        "\n",
        "    Pos embeddings are learnable\n",
        "    \n",
        "    Didnt find information about teacher forcing but ok"
      ],
      "metadata": {
        "id": "Pzf-Ch7iikbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, reduce, repeat\n",
        "from torch import einsum\n",
        "\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "MmLCIEpXimbi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = dict(\n",
        "    num_layers=12,\n",
        "    num_heads=12,\n",
        "    hidden_dim=768,\n",
        "    ffc_hidden_dim=3072,\n",
        ")\n",
        "ffc_activation = 'GELU'\n",
        "max_lr = 2.5e-4"
      ],
      "metadata": {
        "id": "ULTKBLLgiwnE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's create the model from the scratch"
      ],
      "metadata": {
        "id": "4Dmo-lQdlRZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attn_function(q, k, v, mask=None, attn_dropout=None):\n",
        "\n",
        "    #q, k, v shape is [b, s, h]\n",
        "    b_size = q.shape[0]\n",
        "    seq_len = q.shape[1]\n",
        "    hidden_dim = q.shape[2]\n",
        "\n",
        "\n",
        "    scaled_dot_product = einsum('bsh, bvh -> bsv', [q, k])/math.sqrt(hidden_dim)\n",
        "\n",
        "    if mask:\n",
        "        scaled_dot_product = scaled_dot_product.masked_fill(mask==False, 1e-9)\n",
        "\n",
        "    if attn_dropout:\n",
        "        scaled_dot_product = attn_dropout(scaled_dot_product)\n",
        "\n",
        "    attn_probs = F.softmax(scaled_dot_product, dim=-1)\n",
        "    attn_output = einsum('bsv, bvd -> bsd', [attn_probs, v])\n",
        "\n",
        "    return attn_output, attn_probs"
      ],
      "metadata": {
        "id": "nJl-ekSrlVE4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSALayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_heads,\n",
        "                 hidden_dim,\n",
        "                 attn_dropout_p=0.1\n",
        "                 ):\n",
        "\n",
        "        assert hidden_dim % num_heads == 0\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.head_dim = self.hidden_dim // self.num_heads\n",
        "        self.attn_dropout_p = attn_dropout_p\n",
        "\n",
        "        self.toq = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.tok = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.tov = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.ffc = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=self.hidden_dim)\n",
        "        self.attn_dropout = nn.Dropout(p=self.attn_dropout_p if self.attn_dropout_p else 0)\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                mask=None):\n",
        "        # shape of input is [b_size, seq_len, hidden_dim]\n",
        "        q = self.toq(x)\n",
        "        k = self.tok(x)\n",
        "        v = self.tov(x)\n",
        "\n",
        "        q = rearrange(q, 'b s (num_heads h) -> (b num_heads) s h', num_heads=self.num_heads)\n",
        "        k = rearrange(k, 'b s (num_heads h) -> (b num_heads) s h', num_heads=self.num_heads)\n",
        "        v = rearrange(v, 'b s (num_heads h) -> (b num_heads) s h', num_heads=self.num_heads)\n",
        "\n",
        "        output, probs = attn_function(q, k, v, mask=mask, attn_dropout=self.attn_dropout)\n",
        "\n",
        "        output = rearrange(output, '(b num_heads) s h -> b s (num_heads h)', num_heads=self.num_heads)\n",
        "        output = self.ffc(output)\n",
        "\n",
        "        output = self.layer_norm(output + x)\n",
        "        return output, probs\n",
        ""
      ],
      "metadata": {
        "id": "KQvgo4N0opr-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_heads,\n",
        "                 hidden_dim,\n",
        "                 ffc_hidden_dim,\n",
        "                 attn_dropout_p=0.1,\n",
        "                 ffc_dropout_p=0.1,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.ffc_hidden_dim = ffc_hidden_dim\n",
        "        self.attn_dropout_p = attn_dropout_p\n",
        "        self.ffc_dropout_p = ffc_dropout_p\n",
        "\n",
        "        self.ffc_layer = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim, self.ffc_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=self.ffc_dropout_p),\n",
        "            nn.Linear(self.ffc_hidden_dim, self.hidden_dim)\n",
        "        )\n",
        "        self.ffc_layer_norm = nn.LayerNorm(normalized_shape=self.hidden_dim)\n",
        "\n",
        "        self.msalayer = MSALayer(self.num_heads,\n",
        "                                 self.hidden_dim,\n",
        "                                 self.attn_dropout_p,)\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                mask=None):\n",
        "        res = x\n",
        "        x, _ = self.msalayer(x, mask=mask)\n",
        "\n",
        "        return self.ffc_layer_norm(self.ffc_layer(x) + res)"
      ],
      "metadata": {
        "id": "7BuzS4nImxwD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 num_layers,\n",
        "                 num_heads,\n",
        "                 hidden_dim,\n",
        "                 ffc_hidden_dim,\n",
        "                 attn_dropout_p=0.1,\n",
        "                 ffc_dropout_p=0.1,\n",
        "                 max_seq_len=512,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.ffc_hidden_dim = ffc_hidden_dim\n",
        "        self.attn_dropout_p = attn_dropout_p\n",
        "        self.ffc_dropout_p = ffc_dropout_p\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.decoder_block = nn.ModuleList([DecoderLayer(self.num_heads,\n",
        "                                                         self.hidden_dim,\n",
        "                                                         self.ffc_hidden_dim,\n",
        "                                                         self.attn_dropout_p,\n",
        "                                                         self.ffc_dropout_p) for _ in range(self.num_layers)])\n",
        "\n",
        "        self.pos_embeddings = nn.Embedding(self.max_seq_len, self.hidden_dim)\n",
        "        self.token_embeddings = nn.Embedding(self.vocab_size, self.hidden_dim)\n",
        "\n",
        "        self.proj_layer = nn.Linear(self.hidden_dim, self.vocab_size)\n",
        "\n",
        "        self.register_buffer('tril',\n",
        "                             torch.tril(torch.ones(self.max_seq_len, self.max_seq_len)).bool())\n",
        "        self.register_buffer('pos_ids',\n",
        "                             torch.arange(self.max_seq_len))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                input_tokens,\n",
        "                tokenizer_mask=None):\n",
        "        seq_len = input_tokens.shape[-1]\n",
        "        b_size = input_tokens.shape[0]\n",
        "\n",
        "        mask = self.make_attn_mask(seq_len, b_size, tokenizer_mask)\n",
        "\n",
        "        x = self.pos_embeddings(self.pos_ids[:seq_len]) + self.token_embeddings(input_tokens)\n",
        "\n",
        "        for layer in self.decoder_block:\n",
        "            x = layer(x)\n",
        "        x = self.proj_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def make_attn_mask(self, seq_len, b_size, tokenizer_mask=None):\n",
        "        mask = self.tril[:seq_len, :seq_len].unsqueeze(0).repeat(b_size, 1, 1)\n",
        "\n",
        "        if tokenizer_mask is not None:\n",
        "            mask = mask & tokenizer_mask.bool().unsqueeze(1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "yx0Wkst2ryKe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toy_model = GPT(vocab_size=10,\n",
        "                num_layers=2,\n",
        "                num_heads=4,\n",
        "                hidden_dim=100,\n",
        "                ffc_hidden_dim=400,\n",
        "                attn_dropout_p=0.1,\n",
        "                ffc_dropout_p=0.1,\n",
        "                max_seq_len=512,)\n",
        "\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBlq1Lgtzrsu",
        "outputId": "c6e35a8b-3a09-4e25-c3ab-b960d1393833"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes on training\n",
        "* During training we drop last logit, and drop first index of gt_labels"
      ],
      "metadata": {
        "id": "Wlfqt_qERXi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset prep"
      ],
      "metadata": {
        "id": "Z7k8-Am2Sbl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile get_data.sh\n",
        "mkdir data\n",
        "cd data\n",
        "wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5qeb2zqoTxk",
        "outputId": "53549a86-7365-4dd8-8723-058d3b8310e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting get_data.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sh get_data.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY1b3pKVgRak",
        "outputId": "986ec536-9887-4281-9219-1bd8e5d45abe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "--2023-07-20 14:06:41--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "input.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-20 14:06:41 (81.1 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idjKFAj3wHIN",
        "outputId": "1b09e8f3-32c1-4f8c-e252-57adc8f134e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "trainer = BpeTrainer(special_tokens=[\"<unk>\",\n",
        "                                     \"<s>\",\n",
        "                                     \"<pad>\",\n",
        "                                     \"<bos>\",\n",
        "                                     ], vocab_size=10000) #i took 10k just randomly\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "files = [\"data/input.txt\"]\n",
        "tokenizer.train(files, trainer)"
      ],
      "metadata": {
        "id": "ALLvdqa4xBZZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"<bos> $A <s>\",\n",
        "    special_tokens=[\n",
        "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        "        (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n",
        "        (\"<pad>\", tokenizer.token_to_id(\"<pad>\"))\n",
        "    ],\n",
        ")\n",
        "tokenizer.enable_padding(pad_id=2, pad_token=\"<pad>\")\n",
        "tokenizer.save(\"data/tokenizer.json\")"
      ],
      "metadata": {
        "id": "k3lorgYhx10o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode_batch(['Hello I am john Cena', 'Are you?'])[0].tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUAaF-pNx21_",
        "outputId": "aa7eff53-5dc1-4da7-a073-b0e28a3321c2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<bos>', 'He', 'llo', 'I', 'am', 'jo', 'hn', 'C', 'en', 'a', '<s>']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DatasetPreparation"
      ],
      "metadata": {
        "id": "0nf3jI8o2EEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "with open('data/input.txt', 'r') as f:\n",
        "    input_text = f.readlines()\n",
        "input_text = [i for i in input_text if i!='\\n']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mza9BjEX2Gdn",
        "outputId": "c231edcb-f49d-40b1-a655-80c31d1ad0c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['First Citizen:\\n',\n",
              " 'Before we proceed any further, hear me speak.\\n',\n",
              " '\\n',\n",
              " 'All:\\n',\n",
              " 'Speak, speak.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = [i for i in input_text if i!='\\n']\n",
        "print(len(input_text))\n",
        "input_text[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5qC487W2ash",
        "outputId": "00deba4c-74cd-4bf9-dcea-3ee03a185899"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32777\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['First Citizen:\\n',\n",
              " 'Before we proceed any further, hear me speak.\\n',\n",
              " 'All:\\n',\n",
              " 'Speak, speak.\\n',\n",
              " 'First Citizen:\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "class ConstantLenghtDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 texts: List[str],\n",
        "                 tokenizer: Tokenizer,\n",
        "                 length: int=512,):\n",
        "        self.texts = texts\n",
        "        self.length = length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokenizer.no_padding()\n",
        "\n",
        "        encoded_text = tokenizer.encode_batch(self.texts)\n",
        "        tokens_num = [len(s.tokens) for s in encoded_text]\n",
        "        constant_len_dataset_ids = []\n",
        "        concat_sentences_ids = []\n",
        "        sum=0\n",
        "\n",
        "        for idx, num in enumerate(tokens_num):\n",
        "            if sum > 512:\n",
        "                constant_len_dataset_ids.append(concat_sentences_ids)\n",
        "                concat_sentences_ids = []\n",
        "                sum = 0\n",
        "\n",
        "            concat_sentences_ids.append(idx)\n",
        "            sum+=num\n",
        "\n",
        "        np_text = np.array(self.texts)\n",
        "        new_dataset = []\n",
        "        for idxs in constant_len_dataset_ids:\n",
        "            new_dataset.append(' '.join(np_text[idxs].tolist()))\n",
        "\n",
        "        self.dataset = new_dataset\n",
        "\n",
        "    def __len__(self,):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[idx]"
      ],
      "metadata": {
        "id": "6TVmllRF3pAF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizerWrapper():\n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 pad_seq_len=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokenizer.enable_padding(pad_id=2, pad_token=\"<pad>\", length=pad_seq_len)\n",
        "        self.vocab_size = self.tokenizer.get_vocab_size()\n",
        "\n",
        "    def __call__(self, input_sentences: List[str], batch=True):\n",
        "        output = {}\n",
        "        if batch:\n",
        "            encoded_input = self.tokenizer.encode_batch(input_sentences)\n",
        "            ids = torch.tensor([input.ids for input in encoded_input], requires_grad=False)\n",
        "            attn_masks = torch.tensor([input.attention_mask for input in encoded_input], requires_grad=False)\n",
        "        else:\n",
        "            encoded_input = self.tokenizer.encode(input_sentences)\n",
        "            ids = torch.tensor(encoded_input.ids, requires_grad=False).unsqueeze(0)\n",
        "            attn_masks = torch.tensor(encoded_input.attention_mask, requires_grad=False).unsqueeze(0)\n",
        "\n",
        "        output['input_ids'] = ids\n",
        "        output['attn_mask'] = attn_masks\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "cTGsZt_GQTh9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.9\n",
        "train_ids = int(len(input_text) * train_size)\n",
        "train_data = input_text[: train_ids]\n",
        "test_data = input_text[train_ids:]"
      ],
      "metadata": {
        "id": "UkbkfIy-9pqU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ConstantLenghtDataset(train_data, tokenizer, length=512)\n",
        "test_dataset = ConstantLenghtDataset(test_data, tokenizer, length=512)"
      ],
      "metadata": {
        "id": "uYQvCUlsMa34"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes:\n",
        "* Write trainer, which will initialize model, setup tokenizer (pad_len and stuff)\n",
        "* In training remember to take logits up to -1, and preds from 1"
      ],
      "metadata": {
        "id": "0XvyqKyAOmWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple training, just to check how model trains/performs"
      ],
      "metadata": {
        "id": "jaA1ssZA4mis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizerwrapped = TokenizerWrapper(tokenizer, 512)"
      ],
      "metadata": {
        "id": "j-yCYoNoqoCh"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, drop_last=False)\n",
        "\n",
        "\n",
        "model_config = dict(\n",
        "    num_layers=12,\n",
        "    num_heads=12,\n",
        "    hidden_dim=768,\n",
        "    ffc_hidden_dim=3072,\n",
        ")\n",
        "ffc_activation = 'GELU'\n",
        "model = GPT(**model_config, vocab_size=tokenizerwrapped.vocab_size)\n",
        "model = model.cuda()\n",
        "print(torch.cuda.memory_cached()/1e9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTFF0cXHPt8M",
        "outputId": "9fe0359f-78d8-4832-cffd-20b0e1123d31"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.895094272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:416: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z4Xi1G3SV8gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=2e-4)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    training_loss = 0\n",
        "    for batch_num, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = tokenizerwrapped(batch)\n",
        "        labels = inputs['input_ids'].cuda()\n",
        "        logits = model(inputs['input_ids'].cuda(), inputs['attn_mask'].cuda())\n",
        "\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "        loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item()\n",
        "\n",
        "    training_loss /= batch_num\n",
        "    print(f\"Epoch: {epoch}, Training loss: {training_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231,
          "referenced_widgets": [
            "ca0bc58ebdd64a5f9322fb21c6ec9c2a",
            "340c8b2b866b456da6f82e25f6cbe651",
            "060106b072c542aabddb41856367197e",
            "89dfb97724f04a73a7004e02741f14b0",
            "4083e851d32e4026bb49feb70b927d2b",
            "47384ddfbae043cf905a23de0d0732f3",
            "84f6a474e2ac489da49b6eb9c5860be7",
            "bb432215a3b44f22beaaae7ae79b61a2",
            "c09b1ec7c63141d78b617ad4dd753d1d",
            "ea8f8b979b0444ae99ae1417e4b26af6",
            "8535652bbfa94800b17459deee297c0a"
          ]
        },
        "id": "UGtZnjp3kAli",
        "outputId": "75c26f39-2bf0-4107-ad36-d07098ceccde"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca0bc58ebdd64a5f9322fb21c6ec9c2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Training loss: 9.273114935992515\n",
            "Epoch: 1, Training loss: 8.682613830043845\n",
            "Epoch: 2, Training loss: 8.15801719116838\n",
            "Epoch: 3, Training loss: 7.858718414829202\n",
            "Epoch: 4, Training loss: 7.749296305930778\n",
            "Epoch: 5, Training loss: 7.693889892264588\n",
            "Epoch: 6, Training loss: 7.6517268141655075\n",
            "Epoch: 7, Training loss: 7.613897055795748\n",
            "Epoch: 8, Training loss: 7.57659795186291\n",
            "Epoch: 9, Training loss: 7.538805537027855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(batch['input_ids'].cuda(), attn_mask.cuda())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "eb0jTyVnthaz",
        "outputId": "b4c41867-fee6-4f52-aedc-4d82cdf58b80"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-b31d5a624392>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"<bos>\"  # same as above\n",
        "tokenizerwrapped = TokenizerWrapper(tokenizer, 0)\n",
        "batch = tokenizerwrapped(prefix, batch=False)\n",
        "past_key_values = None\n",
        "\n",
        "num_generations = 2000\n",
        "with torch.cuda.amp.autocast():\n",
        "  for i in range(num_generations):\n",
        "\n",
        "    attn_mask = batch['attn_mask']\n",
        "    outputs = model(batch['input_ids'].cuda(), attn_mask.cuda())\n",
        "    probs = outputs[0, -1].div(0.8).softmax(-1)\n",
        "    token = torch.multinomial(probs, 1).view([])\n",
        "\n",
        "    print(tokenizerwrapped.tokenizer.decode([token]), end=' ', flush=True)\n",
        "    batch = dict(input_ids=outputs[0, -1].argmax(-1).reshape(1, 1),\n",
        "                 attn_mask=torch.ones(num_generations, requires_grad=False).cuda()[:i+1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0dBlgi8qMow",
        "outputId": "333aa843-8c18-4d89-ec81-71cf28faca67"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quarters                                                                                                                                     rain                                                                                                                          ival                                                                                                                                                                                                                                                                                                                                                                                               Mars                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               tire             oracle                                             spy    grew                 liments                                                                                                                                                                                                    erest                                                                                                                      any                                                                             INE                                                                                                          blaze                                                                                                                             honourable                IO                                      "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"<bos>\"  # same as above\n",
        "tokenizerwrapped = TokenizerWrapper(tokenizer, 0)\n",
        "batch = tokenizerwrapped(prefix, batch=False)"
      ],
      "metadata": {
        "id": "B1Og2ajJrdMg"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kin00vR6sFJY",
        "outputId": "1e27cbc9-c19f-43c7-a2b0-b16dddaa0af8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[3, 3, 1]]), 'attn_mask': tensor([[1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vvh4VtEFsxNj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}