{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-19T16:28:33.645834Z","iopub.execute_input":"2023-07-19T16:28:33.646442Z","iopub.status.idle":"2023-07-19T16:28:37.366108Z","shell.execute_reply.started":"2023-07-19T16:28:33.646401Z","shell.execute_reply":"2023-07-19T16:28:37.364889Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2023-07-19T16:28:37.368015Z","iopub.execute_input":"2023-07-19T16:28:37.368630Z","iopub.status.idle":"2023-07-19T16:28:50.124211Z","shell.execute_reply.started":"2023-07-19T16:28:37.368587Z","shell.execute_reply":"2023-07-19T16:28:50.122888Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.6.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from einops import rearrange","metadata":{"execution":{"iopub.status.busy":"2023-07-19T16:28:50.126635Z","iopub.execute_input":"2023-07-19T16:28:50.127023Z","iopub.status.idle":"2023-07-19T16:28:50.141511Z","shell.execute_reply.started":"2023-07-19T16:28:50.126983Z","shell.execute_reply":"2023-07-19T16:28:50.140485Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Explanation\n\nThis Seq2Seq part of architecture expects as input tensor of shape 'b_size, seq_len, hidden_dim', where:\n* b_size -> batch size\n* seq_len -> frame sequence length\n* hidden_dim -> flattened output of backbone net (like ResNet and etc) (b w h c -> b (w h c))\n\nReturns:\n\n    logits of shape (b_size, seq_len), with 0/1 probabilities","metadata":{}},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"X = torch.randn((10000, 60, 1000), requires_grad=False) #b_size, seq_len, hidden_dim\ny = torch.randint(2,(10000, 60), requires_grad=False, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T16:28:50.144913Z","iopub.execute_input":"2023-07-19T16:28:50.145298Z","iopub.status.idle":"2023-07-19T16:28:55.548992Z","shell.execute_reply.started":"2023-07-19T16:28:50.145263Z","shell.execute_reply":"2023-07-19T16:28:55.548033Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n\nclass SimpleDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def __len__(self,):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n    \n    \ntrain_dataset = SimpleDataset(X, y)\ntrain_dataloader = DataLoader(train_dataset, batch_size=64,  shuffle=True, drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T16:28:55.550480Z","iopub.execute_input":"2023-07-19T16:28:55.551148Z","iopub.status.idle":"2023-07-19T16:28:55.561168Z","shell.execute_reply.started":"2023-07-19T16:28:55.551113Z","shell.execute_reply":"2023-07-19T16:28:55.560271Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self,\n                 hidden_dim,\n                 num_layers,\n                 encoder_dropout: float=0.5):\n        super().__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.encoder_dropout = encoder_dropout\n        \n        \n        self.encoder = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=encoder_dropout)\n        \n        \n    def forward(self, x):\n        return self.encoder(x)  #output: y-pred, h, c","metadata":{"execution":{"iopub.status.busy":"2023-07-19T16:28:55.564775Z","iopub.execute_input":"2023-07-19T16:28:55.566478Z","iopub.status.idle":"2023-07-19T16:28:55.577792Z","shell.execute_reply.started":"2023-07-19T16:28:55.566444Z","shell.execute_reply":"2023-07-19T16:28:55.576904Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from typing import Tuple\nclass Decoder(nn.Module):\n    def __init__(self,\n                hidden_dim,\n                num_layers,\n                num_classes,\n                decoder_dropout: float=0.5):\n        super().__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.num_classes = num_classes\n        self.decoder_dropout = decoder_dropout\n        \n        self.decoder = nn.LSTM(self.num_classes, hidden_dim, num_layers, batch_first=True, dropout=decoder_dropout)\n        self.proj_layer = nn.Linear(hidden_dim, 1)\n        \n    def forward(self,\n                x: torch.Tensor,\n                hc: Tuple[torch.Tensor, torch.Tensor]):\n        #Decoder is auto regressive, gets x with shape b_size, 1, hidden_dim\n        #Outputs prediction for the current input and hidden, cell\n        assert x.dim() == 1                  #we have X as a vector of shape [b_size]\n        x = rearrange(x, 'b -> b 1 1')       #adding seq_len, hidden_dim dimension, x now is [b_size, 1, hidden_dim] \n        \n        out, hc = self.decoder(x, hc)\n        out = rearrange(out, 'b 1 h -> b h')                     #removing seq_len & hidden_dim dimension\n        out = self.proj_layer(out) \n        out = rearrange(out, 'b 1 -> b')\n        return out, hc\n                ","metadata":{"execution":{"iopub.status.busy":"2023-07-19T16:28:55.581360Z","iopub.execute_input":"2023-07-19T16:28:55.582464Z","iopub.status.idle":"2023-07-19T16:28:55.596979Z","shell.execute_reply.started":"2023-07-19T16:28:55.582431Z","shell.execute_reply":"2023-07-19T16:28:55.596095Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import random \nclass Seq2Seq(nn.Module):\n    def __init__(self,\n                 encoder: torch.nn.Module,\n                 decoder: torch.nn.Module,):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def forward(self,\n               src, \n               target,\n               teacher_force: float=0.5):\n        b_size = src.shape[0]\n        max_seq_len = src.shape[1]\n        \n        \n        _, hc = self.encoder(src)\n        \n        input_ = target[:, 0]\n        output = torch.zeros((b_size, max_seq_len), requires_grad=False).to(device)\n        \n        for t in range(1, max_seq_len):\n            out, hc = self.decoder(input_, hc)\n            output[:, t] = out\n            y_pred = (out > 0.5).float()\n            y_pred\n            \n            do_tf = random.random() > teacher_force\n            \n            \n            if do_tf:\n                input_ = target[:, t]\n            else:\n                input_ = y_pred\n                \n        return output\n            \n            \n            ","metadata":{"execution":{"iopub.status.busy":"2023-07-19T16:33:44.743559Z","iopub.execute_input":"2023-07-19T16:33:44.743924Z","iopub.status.idle":"2023-07-19T16:33:44.755334Z","shell.execute_reply.started":"2023-07-19T16:33:44.743894Z","shell.execute_reply":"2023-07-19T16:33:44.754305Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"device = 'cuda'\nenc = Encoder(1000, 2)\ndec = Decoder(1000, 2, 1)\nmodel = Seq2Seq(enc, dec).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nloss_fn = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2023-07-19T16:33:44.924757Z","iopub.execute_input":"2023-07-19T16:33:44.925282Z","iopub.status.idle":"2023-07-19T16:33:45.190250Z","shell.execute_reply.started":"2023-07-19T16:33:44.925220Z","shell.execute_reply":"2023-07-19T16:33:45.189257Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"X.device","metadata":{"execution":{"iopub.status.busy":"2023-07-19T16:33:45.191920Z","iopub.execute_input":"2023-07-19T16:33:45.192279Z","iopub.status.idle":"2023-07-19T16:33:45.198861Z","shell.execute_reply.started":"2023-07-19T16:33:45.192229Z","shell.execute_reply":"2023-07-19T16:33:45.197924Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"num_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    for batch_num, batch in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        src, tgt = batch[0].to(device), batch[1].to(device)\n        y_pred = model(src, tgt)\n        \n        loss = loss_fn(y_pred, tgt)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n    train_loss /= batch_num\n    print(f'Loss: {train_loss}')\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2023-07-19T16:33:52.842381Z","iopub.execute_input":"2023-07-19T16:33:52.842751Z","iopub.status.idle":"2023-07-19T16:37:20.704575Z","shell.execute_reply.started":"2023-07-19T16:33:52.842721Z","shell.execute_reply":"2023-07-19T16:37:20.703175Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Loss: 0.702020084246611\nLoss: 0.7030257131808844\nLoss: 0.7015808408076947\nLoss: 0.701946924512203\nLoss: 0.7060159918589469\nLoss: 0.7023547658553491\nLoss: 0.7012652089962592\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(src, tgt)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, tgt)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}